import streamlit as st
import requests
import logging
from typing import List, Optional
from pydantic import BaseModel
from datetime import datetime

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('chat.log'),
        logging.StreamHandler()
    ]
)

# Ollama API è¨­ç½®
OLLAMA_API_BASE = "http://localhost:11434"

def get_available_models():
    """ç²å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    try:
        response = requests.get(f"{OLLAMA_API_BASE}/api/tags")
        if response.status_code == 200:
            models = [model['name'] for model in response.json()['models']]
            logging.info(f"ç²å–åˆ°çš„æ¨¡å‹åˆ—è¡¨: {models}")
            return models
        logging.error(f"ç²å–æ¨¡å‹åˆ—è¡¨å¤±æ•—: HTTP {response.status_code}")
        return []
    except Exception as e:
        logging.error(f"ç„¡æ³•ç²å–æ¨¡å‹åˆ—è¡¨: {str(e)}")
        st.error(f"ç„¡æ³•ç²å–æ¨¡å‹åˆ—è¡¨: {str(e)}")
        return []

def chat_with_model(model: str, prompt: str) -> str:
    """èˆ‡é¸å®šçš„æ¨¡å‹å°è©±"""
    try:
        logging.info(f"é–‹å§‹èˆ‡æ¨¡å‹ {model} å°è©±")
        logging.info(f"ç”¨æˆ¶è¼¸å…¥: {prompt}")
        
        response = requests.post(
            f"{OLLAMA_API_BASE}/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "stream": False
            }
        )
        
        if response.status_code == 200:
            result = response.json()['response']
            logging.info(f"æ¨¡å‹ {model} å›æ‡‰: {result[:100]}...")  # åªè¨˜éŒ„å‰100å€‹å­—ç¬¦
            return result
            
        logging.error(f"æ¨¡å‹å›æ‡‰éŒ¯èª¤: HTTP {response.status_code}")
        return f"éŒ¯èª¤: {response.status_code}"
    except Exception as e:
        error_msg = f"èˆ‡æ¨¡å‹ {model} å°è©±æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        logging.error(error_msg)
        return f"éŒ¯èª¤: {str(e)}"

st.set_page_config(
    page_title="LLM Chat",
    page_icon="ğŸ’¬",
    layout="wide"
)

st.title("LLM Chat Interface")

# åˆå§‹åŒ–èŠå¤©æ­·å²
if "messages" not in st.session_state:
    st.session_state.messages = []

# å´é‚Šæ¬„ï¼šæ¨¡å‹é¸æ“‡
with st.sidebar:
    st.title("è¨­ç½®")
    available_models = get_available_models()
    if available_models:
        selected_model = st.selectbox(
            "é¸æ“‡æ¨¡å‹",
            available_models,
            index=0 if available_models else None
        )
    else:
        st.error("ç„¡æ³•ç²å–æ¨¡å‹åˆ—è¡¨ã€‚è«‹ç¢ºä¿ Ollama æœå‹™æ­£åœ¨é‹è¡Œã€‚")
        selected_model = None

    # æ–‡ä»¶ä¸Šå‚³åŠŸèƒ½
    uploaded_file = st.file_uploader("ä¸Šå‚³æ–‡ä»¶", type=["txt", "pdf", "doc", "docx"])
    if uploaded_file is not None:
        st.success(f"æ–‡ä»¶ '{uploaded_file.name}' ä¸Šå‚³æˆåŠŸï¼")
        # TODO: å¯¦ç¾æ–‡ä»¶è™•ç†é‚è¼¯

# é¡¯ç¤ºèŠå¤©æ­·å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# èŠå¤©è¼¸å…¥
if prompt := st.chat_input("è¼¸å…¥æ‚¨çš„è¨Šæ¯..."):
    if not selected_model:
        st.error("è«‹å…ˆé¸æ“‡ä¸€å€‹æ¨¡å‹")
        logging.warning("ä½¿ç”¨è€…å˜—è©¦åœ¨æœªé¸æ“‡æ¨¡å‹çš„æƒ…æ³ä¸‹ç™¼é€è¨Šæ¯")
    else:
        # æ·»åŠ ç”¨æˆ¶è¨Šæ¯
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # ç²å–æ¨¡å‹å›æ‡‰
        with st.spinner('æ€è€ƒä¸­...'):
            response = chat_with_model(selected_model, prompt)

        # æ·»åŠ åŠ©æ‰‹å›æ‡‰
        st.session_state.messages.append({"role": "assistant", "content": response})
        with st.chat_message("assistant"):
            st.markdown(response)

class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[Message]
    context: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    sources: Optional[List[str]] = None
