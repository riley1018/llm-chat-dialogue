import streamlit as st
import requests
import logging
from typing import List, Optional
from pydantic import BaseModel
from datetime import datetime

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('chat.log'),
        logging.StreamHandler()
    ]
)

# Ollama API è¨­ç½®
OLLAMA_API_BASE = "http://localhost:11434"

def get_available_models():
    """ç²å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    try:
        response = requests.get(f"{OLLAMA_API_BASE}/api/tags")
        if response.status_code == 200:
            models = [model['name'] for model in response.json()['models']]
            logging.info(f"ç²å–åˆ°çš„æ¨¡å‹åˆ—è¡¨: {models}")
            return models
        logging.error(f"ç²å–æ¨¡å‹åˆ—è¡¨å¤±æ•—: HTTP {response.status_code}")
        return []
    except Exception as e:
        logging.error(f"ç„¡æ³•ç²å–æ¨¡å‹åˆ—è¡¨: {str(e)}")
        st.error(f"ç„¡æ³•ç²å–æ¨¡å‹åˆ—è¡¨: {str(e)}")
        return []

def chat_with_model(model: str, prompt: str) -> str:
    """èˆ‡é¸å®šçš„æ¨¡å‹å°è©±"""
    try:
        logging.info(f"é–‹å§‹èˆ‡æ¨¡å‹ {model} å°è©±")
        logging.info(f"ç”¨æˆ¶è¼¸å…¥: {prompt}")
        
        response = requests.post(
            f"{OLLAMA_API_BASE}/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "stream": False
            }
        )
        
        if response.status_code == 200:
            result = response.json()['response']
            logging.info(f"æ¨¡å‹ {model} å›æ‡‰: {result[:100]}...")  # åªè¨˜éŒ„å‰100å€‹å­—ç¬¦
            return result
            
        logging.error(f"æ¨¡å‹å›æ‡‰éŒ¯èª¤: HTTP {response.status_code}")
        return f"éŒ¯èª¤: {response.status_code}"
    except Exception as e:
        error_msg = f"èˆ‡æ¨¡å‹ {model} å°è©±æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        logging.error(error_msg)
        return f"éŒ¯èª¤: {str(e)}"

st.set_page_config(
    page_title="LLM Chat",
    page_icon="ğŸ’¬",
    layout="wide"
)

st.title("LLM Chat Interface")

# åˆå§‹åŒ–èŠå¤©æ­·å²
if "messages" not in st.session_state:
    st.session_state.messages = []

# å´é‚Šæ¬„ï¼šæ¨¡å‹é¸æ“‡
with st.sidebar:
    st.title("è¨­ç½®")
    available_models = get_available_models()
    if available_models:
        selected_model = st.selectbox(
            "é¸æ“‡æ¨¡å‹",
            available_models,
            index=0 if available_models else None
        )
    else:
        st.error("ç„¡æ³•ç²å–æ¨¡å‹åˆ—è¡¨ã€‚è«‹ç¢ºä¿ Ollama æœå‹™æ­£åœ¨é‹è¡Œã€‚")
        selected_model = None

    # åˆå§‹åŒ– GraphRAG
    if "graph_rag" not in st.session_state:
        from graph_rag import GraphRAG
        st.session_state.graph_rag = GraphRAG()

    # æ–‡ä»¶ä¸Šå‚³åŠŸèƒ½
    uploaded_file = st.file_uploader("ä¸Šå‚³CSVæ–‡ä»¶", type=["csv"])
    if uploaded_file is not None:
        try:
            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
            import tempfile
            import os
            
            with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_path = tmp_file.name
            
            # å¤„ç†CSVæ–‡ä»¶
            st.session_state.graph_rag.process_csv(tmp_path)
            
            # è·å–å›¾è°±ç»Ÿè®¡ä¿¡æ¯
            stats = st.session_state.graph_rag.get_graph_statistics()
            st.success(f"æ–‡ä»¶ '{uploaded_file.name}' è™•ç†æˆåŠŸï¼")
            st.info(f"çŸ¥è­˜åœ–è­œçµ±è¨ˆï¼š\n- ç¯€é»æ•¸ï¼š{stats['num_nodes']}\n- é‚Šæ•¸ï¼š{stats['num_edges']}\n- ç¯€é»é¡å‹æ•¸ï¼š{stats['node_types']}")
            
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            os.unlink(tmp_path)
            
        except Exception as e:
            st.error(f"è™•ç†æ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")

# é¡¯ç¤ºèŠå¤©æ­·å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# èŠå¤©è¼¸å…¥
if prompt := st.chat_input("è¼¸å…¥æ‚¨çš„è¨Šæ¯..."):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²è®°å½•
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # æ£€æŸ¥æ˜¯å¦å·²ä¸Šä¼ å¹¶å¤„ç†äº†CSVæ–‡ä»¶
    if "graph_rag" in st.session_state:
        try:
            # ä½¿ç”¨GraphRAGè¿›è¡ŒæŸ¥è¯¢
            rag_results = st.session_state.graph_rag.query(prompt)
            
            # æ„å»ºå¸¦æœ‰ä¸Šä¸‹æ–‡çš„æç¤º
            context = "åŸºæ–¼ä»¥ä¸‹ç›¸é—œä¿¡æ¯å›ç­”ï¼š\n"
            for ctx in rag_results['relevant_contexts']:
                context += f"- ä¸»è¦ä¿¡æ¯ï¼š{ctx['node']}\n"
                context += f"- ç›¸é—œä¿¡æ¯ï¼š{', '.join(str(n) for n in ctx['neighbors'])}\n"
            
            enhanced_prompt = f"{context}\nç”¨æˆ¶å•é¡Œï¼š{prompt}"
            
        except Exception as e:
            st.error(f"æŸ¥è©¢çŸ¥è­˜åœ–è­œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
            enhanced_prompt = prompt
    else:
        enhanced_prompt = prompt

    # è·å–æ¨¡å‹å›åº”
    if selected_model:
        with st.chat_message("assistant"):
            response = chat_with_model(selected_model, enhanced_prompt)
            st.session_state.messages.append({"role": "assistant", "content": response})
            st.markdown(response)
    else:
        st.error("è«‹å…ˆé¸æ“‡ä¸€å€‹æ¨¡å‹")

class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[Message]
    context: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    sources: Optional[List[str]] = None
